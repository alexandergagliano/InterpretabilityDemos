{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5e3f06-527c-46c4-9f09-c0bfca98c382",
   "metadata": {},
   "source": [
    "# 4.2.  Counterfactual Explanations\n",
    "### Alex Gagliano (gaglian2@mit.edu)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandergagliano/InterpretabilityDemos/blob/main/Tutorial%204.2%20Counterfactual.ipynb)\n",
    "\n",
    "References and resources for additional reading:\n",
    "* [Alibi Explain: Algorithms for Explaining Machine Learning Models](https://github.com/SeldonIO/alibi) (Klaise, Van Looveren, Vacanti, & Alexandru Coc, 2017)\n",
    "* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/), Christoph Molnar\n",
    "* [Machine Learning Tutorials](https://github.com/MichelleLochner/ml-tutorials/tree/main), Michelle Lochner\n",
    "* [Interpretable Counterfactual Explanations Guided by Prototypes](https://arxiv.org/abs/1907.02584), (Van Looveren & Klaise, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5789e1-703f-45f2-a4bc-a8ae06a6a5a9",
   "metadata": {},
   "source": [
    "We might describe an \"interpretable model\" as one in which it is clear how input maps to output. We looked at SHAP values to better understand what features might have \"pushed\" a result in a particular direction, but another way to explore this mapping is to change the inputs and evaluate its impact on the outputs. \n",
    "\n",
    "In this notebook, we'll explore multi-class classification. We have examples in our training class belonging to a given class. Take an image of a spiral galaxy, for example. What do I have to modify in that image, and by how much, before my model believes that it's no longer a spiral galaxy? An example that answers this question is called a \"counterfactual\", which describes the smallest change to a set of feature values that will flip the predicted output. Exploring counterfactuals helps us understand what the 'boundaries' of our decision-making model are, which in turn helps us understand what features are important for decision-making. \n",
    "\n",
    "For a given set of input features $x$, a counterfactual $x'$ can be found by minimizing a loss function of the following general form:\n",
    "$$ L(x'|x) = (f_t(x') - p_t)^2 + \\lambda d(x, x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc2da0-aeba-4a6d-b075-f454e3cda2a0",
   "metadata": {},
   "source": [
    "Where $f_t(x')$ describes the class probability of counterfactual with features $x'$, $p_t$ defines the target class probability (how confident we want the new prediction to be), $\\lambda$ is a regularization parameter that is determined during fitting, and $d(x, x')$ is some distance measure between feature sets $x$ and $x'$ (Manhattan, Euclidean, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114fa3d-c478-481d-8049-da99aeed488d",
   "metadata": {},
   "source": [
    "Counterfactuals, like SHAP values, are another example of a _local_ explanation model - we investigate the complex model's behavior at and near a specific set of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849a1a0-64bb-4686-b29f-d4e8849263be",
   "metadata": {},
   "source": [
    "We'll use the `alibi` package, which minimizes the above loss function and finds our counterfactuals using simple gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae253261-fad7-4d0a-b863-1a1f506955a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi torch torchvision gdown tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da6dcd-7e1f-45bb-912d-a456f08654ba",
   "metadata": {},
   "source": [
    "## 4.2.1. Image counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c30595-c7a8-47da-950e-65e56cd23294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "from alibi.explainers import Counterfactual, CounterfactualProto\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from tqdm import trange\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490d85e-c53e-46db-b551-4d64de4bca5b",
   "metadata": {},
   "source": [
    "As before, we download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73615bf9-a983-42d2-9a21-3688360f7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1s42ri7tpvBHN-kneC0MHXHsd2nB2BNTE\n",
    "!gdown 1CC07axEZravXcIkq0w2gnkFGGhO4vbnx\n",
    "\n",
    "for file in ['data', 'models']:\n",
    "    subprocess.run([\"tar\", \"-xf\", \"%s.tar.gz\" % file]);\n",
    "    \n",
    "galaxyPath = './data/galaxy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f30d8-fe49-4a19-ba62-23ddf8ec096c",
   "metadata": {},
   "source": [
    "Create our data loaders as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df816a56-d905-428d-aae8-2fe21d44621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(23)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.ToTensor()])\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(os.path.join(galaxyPath, 'train'), transform = data_transform)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_size, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "test = torchvision.datasets.ImageFolder(os.path.join(galaxyPath, 'test'), transform = data_transform)\n",
    "test_loader = DataLoader(test, shuffle=True, batch_size=batch_size, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccd11f-8dec-432a-b8ab-792127402899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 20, kernel_size=5),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(720, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(50, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3*16*15)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df94bbe-fc0a-4e60-b816-bd7c459b9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = Net().to(device)\n",
    "\n",
    "#load the model from the SHAP tutorial!\n",
    "model.load_state_dict(torch.load('./models/galaxyCNN_legacy.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d686d73-cd51-4643-a8c9-3603525f8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Note! A pytorch model's forward function returns a tensor and not a numpy array. \n",
    "# We have to do slightly more work to get the counterfactual package to play nicely with\n",
    "# the model we've trained.\n",
    "@torch.no_grad()\n",
    "def predictor(X: np.ndarray) -> np.ndarray:\n",
    "    X = torch.as_tensor(X,dtype=torch.float, device=device)\n",
    "    return model.forward(X).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365fa20-dfff-4789-80bb-6c0aed1aac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the next batch from the dataset\n",
    "image, label = next(iter(test_loader)) \n",
    "X = image[6:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8035104-b53d-4c5a-80c7-2bc5213583b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here our instance of choice is the following image:\n",
    "plt.imshow(X.squeeze(0,1), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1f6e4-84ce-4d7a-9522-7352ca8453d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prediction = torch.max(model(X), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08426a-c6b7-425e-b46b-a7bc7f935444",
   "metadata": {},
   "outputs": [],
   "source": [
    "GalaxyClasses = {0:'Ellip.', 1:'Spiral', 2:'Irreg.'}\n",
    "\n",
    "predClass = GalaxyClasses[prediction.item()]\n",
    "trueClass = GalaxyClasses[label[6].item()]\n",
    "\n",
    "print(\"The predicted class is %s.\" % predClass)\n",
    "print(\"The true class is %s.\" % trueClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc202d-d140-49df-bf35-5cb7e6bd6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# initialize explainer\n",
    "shape = (1,) + image.shape[1:]\n",
    "\n",
    "cf = Counterfactual(predictor, shape, distance_fn='l1', target_proba=1.0,\n",
    "                    target_class='other', max_iter=100, early_stop=50, lam_init=1e-1,\n",
    "                    max_lam_steps=10, tol=0.05, learning_rate_init=0.1,\n",
    "                    feature_range=(0, 1), eps=0.01, init='identity',\n",
    "                    decay=True, write_dir=None, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7656d-c79d-42a0-99b4-e14b93657f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "explanation = cf.explain(X);\n",
    "print('Explanation took {:.3f} sec.'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c790ab-5d10-4005-b378-b6fafa47f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = explanation.cf['class']\n",
    "proba = explanation.cf['proba'][0][pred_class]\n",
    "\n",
    "print(f'Counterfactual prediction: {GalaxyClasses[pred_class]} with probability {proba}')\n",
    "plt.imshow(explanation.cf['X'].reshape(32, 32), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca0f0d-14e3-47c5-b8a0-3aeae6aae548",
   "metadata": {},
   "source": [
    "## Challenge: Consider the ensemble case. Try transforming the image (rotation, flip, slightly changing pixel values) and generating additional counterfactual examples. What does that tell us about what our model is sensitive to?\n",
    "Some helpful functions are `torch.rot90` and `torch.flip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab5f05-77a3-4320-b1d8-5f1acddd7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - generate a series of flipped and rotated images. \n",
    "# Generate counterfactuals in each case and compare. \n",
    "# Now slightly increase or decrease individual pixel values. \n",
    "# Do the generated counterfactuals change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed9141-7631-43b2-b08d-8497d75cf873",
   "metadata": {},
   "source": [
    "### This turns out to be a disappointingly common phenomenon and has given rise to [one-pixel attacks](https://arxiv.org/abs/1710.08864), to which many deep networks are vulnerable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ce1a1-8042-45df-a421-e1718f1cabe8",
   "metadata": {},
   "source": [
    "The example seems trivial but the insight doesn't need to be. It looks like the model learned that irregular galaxies often have bright spots off-center in their respective images, though the model doesn't understand what a _physical_ bright spot looks like (no point-spread function here!).  We could continue to re-run for additional counterfactuals and confirm that's what the network considers archetypal for its class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53014998-6b84-4d93-aeee-2bd3a1b46e1d",
   "metadata": {},
   "source": [
    "During the fitting technique, the regularization parameter $\\lambda$ is slowly changed to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ceb10-d71a-432e-8db8-72fd674d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cfs = np.array([len(explanation.all[iter_cf]) for iter_cf in range(10)])\n",
    "examples = {}\n",
    "for ix, n in enumerate(n_cfs):\n",
    "    if n>0:\n",
    "        examples[ix] = {'ix': ix, 'lambda': explanation.all[ix][0]['lambda'],\n",
    "                       'X': explanation.all[ix][0]['X']}\n",
    "columns = len(examples) + 1\n",
    "rows = 1\n",
    "\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "for i, key in enumerate(examples.keys()):\n",
    "    ax = plt.subplot(rows, columns, i+1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.imshow(examples[key]['X'].reshape(32,32))\n",
    "    lam = examples[key]['lambda']\n",
    "    plt.title(f'Iter {key}, Lambda {lam:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5629f41-c410-4ed4-b753-d82fe4c793f0",
   "metadata": {},
   "source": [
    "Each of these is technically a counterfactual of the original image, but with slightly different regularization parameters determining how similar the new features $x'$ are to the input features $x$. Now let's see what it takes to point the counterfactual toward a specific class - this time we'll aim for creating an elliptical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b95ec-df3f-4a0f-8507-7f409c1dbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 0 # elliptical galaxy\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "cf = Counterfactual(predictor, shape, distance_fn='l1', target_proba=1.0,\n",
    "                    target_class=target_class, max_iter=100, early_stop=50, lam_init=1e-1,\n",
    "                    max_lam_steps=10, tol=0.05, learning_rate_init=0.01,\n",
    "                    feature_range=(0, 1), eps=0.01, init='identity',\n",
    "                    decay=True, write_dir=None, debug=False)\n",
    "\n",
    "explanation = start_time = time()\n",
    "explanation = cf.explain(X)\n",
    "print('Explanation took {:.3f} sec'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c609e-1958-4aa1-871e-e08fb90620a3",
   "metadata": {},
   "source": [
    "Our counterfactual is now the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafd0af-d613-4f0f-938d-f4727b95fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(explanation.cf['X'].reshape(32, 32), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114dbbe-7d1e-4bf2-8022-14cd84dc1b95",
   "metadata": {},
   "source": [
    "And to confirm that this would have indeed been classified as an elliptical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa246f-2f61-4237-99d8-5849fb3af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = explanation.cf['class']\n",
    "\n",
    "print(f'Counterfactual prediction: {GalaxyClasses[pred_class]} with probability {proba}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f66e1-665f-4da1-a497-e264488a1889",
   "metadata": {},
   "source": [
    "It looks like a bright central region is archetypal of an elliptical, though the structure in the image leaves a fair bit to be desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218eade-37a4-48cd-b228-8cd96ea7a5ba",
   "metadata": {},
   "source": [
    "## 4.2.2. Time-series Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8003b9-d14b-4250-8e63-645be763eaf3",
   "metadata": {},
   "source": [
    "Next, we'll explore counterfactuals from the realm of supernova science. Supernovae are the explosive deaths of stars, and we detect these phenomena as transient events in data from (typically) optical telescopes. As our discovery rates have grown, we've resorted to characterizing these events by photometry alone - spectroscopic follow-up is too resource-intensive to study everything. The large associated data volumes and the subtle differences between photometric samples makes this a natural case study for machine learning classification tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a228c-27c7-4af0-8114-300f601382a4",
   "metadata": {},
   "source": [
    "Let's first look at some simulated supernova photometry. These data are obtained from the Photometric Supernova Classification Challenge (SPCC, Kessler et al. 2010), and presented in $griz$ passbands from the Dark Energy Survey (DES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dc3f2-ccf9-404f-b30b-f6653ada7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sncosmo iminuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdeb51-11ae-4a09-a373-a982c168301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sncosmo\n",
    "from astropy.table import Table\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8f7cb-402f-42de-897e-6e103fd56216",
   "metadata": {},
   "outputs": [],
   "source": [
    "transientPath = './data/supernova/lightcurves/'\n",
    "\n",
    "lc_file = os.path.join(transientPath, '34.dat')\n",
    "lc = Table.read(lc_file, format='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef383983-7c76-4de4-88bf-52bb7682d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f5539-2063-4a65-b04f-70eb53cda179",
   "metadata": {},
   "source": [
    "A common pre-processing step is `feature extraction`: a model is fit to the supernova photometry and classification is done with these model features. Here, we'll fit a light curve to SALT2 (Guy et al., 2007), which was created for type Ia supernovae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff26d57-52a2-410d-ae95-e43d96b79df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_supernova(lc):\n",
    "    \"\"\"\n",
    "    Small function to fit a light curve with the SALT2 model, using sncosmo and iminuit.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    lc : astropy.table.Table\n",
    "        Light curve (in the format sncosmo expects)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    z, t0, x0, x1, c\n",
    "        Best-fitting parameters of the model\n",
    "    \"\"\"\n",
    "    bnds = {'z':(0.01, 1.5), 't0':(-100,100),'x0':(-1e-3, 1e-3), 'x1':(-3, 3), 'c':(-0.5, 0.5)}\n",
    "    mod = sncosmo.Model('salt2-extended')\n",
    "    res = sncosmo.fit_lc(lc, mod, vparam_names=mod.param_names, bounds=bnds, minsnr=0)\n",
    "    return res[0].parameters\n",
    "\n",
    "prms = fit_supernova(lc)\n",
    "print('Best fitting SALT2 parameters: [z, t0, x0, x1, c]:')\n",
    "print(prms)\n",
    "\n",
    "mod = sncosmo.Model('salt2-extended')\n",
    "mod.parameters = prms\n",
    "sncosmo.plot_lc(lc, mod);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1ab51-1dea-437f-88cf-d1d2f2a619e8",
   "metadata": {},
   "source": [
    "The SALT2 features for all supernovae in this sample have been pre-computed, and the classifications are given (1 = Ia, 2 = II, 3 = Ibc). Let's construct a basic Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9185e-89e0-488f-aa73-25e1a409dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNclasses = {1:'SN Ia', 2:'SN II', 3:'SN Ibc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d717d3-b7f4-4183-9200-eb8d5dc01338",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt2_features = np.loadtxt(os.path.join('data/supernova/','salt2_features.txt'), comments='#')\n",
    "labels = np.loadtxt(os.path.join('data/supernova/','labels.txt'), comments='#')\n",
    "\n",
    "SS = StandardScaler()\n",
    "X = SS.fit_transform(salt2_features[:, 1:])\n",
    "y = labels[:, 1].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#re-balance the training set: \n",
    "under = RandomUnderSampler(random_state=42, sampling_strategy={2: 500, 1: 404, 3: 190})\n",
    "over = RandomOverSampler(random_state=42, sampling_strategy={2: 500, 1: 500, 3: 500})\n",
    "steps = [('u', under), ('o', over)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X_train_res, y_train_res = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8699d8b-3521-4b6c-8468-f1dba9a9cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Support Vector Machine Classifier\n",
    "clf = svm.SVC(kernel='rbf', probability=True) \n",
    "\n",
    "#Train the model\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2df05-b7fc-4946-ad04-4f85d10875b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=SNclasses.values())\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b3daa-561c-4615-9ead-a882ef1379e8",
   "metadata": {},
   "source": [
    "Not horrible. Now let's compute some counterfactuals for a single example. First looking at the event for which we'd like a counterfactual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebdce5-e821-4ab1-b231-61f72f7508d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "lc_file = os.path.join(transientPath, '%i.dat'%idx)\n",
    "lc = Table.read(lc_file, format='ascii')\n",
    "\n",
    "mod_orig = sncosmo.Model('salt2-extended')\n",
    "mod_orig.parameters = salt2_features[idx][1:]\n",
    "sncosmo.plot_lc(lc, mod_orig);\n",
    "\n",
    "print(\"Event is an %s.\" % SNclasses[labels[idx, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfc681-9eeb-4b89-ad95-35dc660d6fdb",
   "metadata": {},
   "source": [
    "As before, let's generate a counterfactual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec0183-261e-4b6e-95e8-2ec4a325c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = lambda x: clf.predict_proba(x)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "shape = (1,) + X[idx].reshape(1, -1).shape[1:]\n",
    "\n",
    "cf = Counterfactual(predictor, shape, distance_fn='l1', target_proba=1.0,\n",
    "                    target_class=1, max_iter=1000, lam_init=1e-5,\n",
    "                    max_lam_steps=50, tol=0.05, learning_rate_init=0.1,\n",
    "                    feature_range=(-1.e5, 1.e5), eps=0.01, init='identity',\n",
    "                    decay=True, write_dir=None, debug=False)\n",
    "\n",
    "explanation = start_time = time()\n",
    "explanation = cf.explain(X[idx].reshape(1, -1))\n",
    "print('Explanation took {:.3f} sec'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f21a2a-7908-4a3f-b801-a2bfb6bcd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_counter = sncosmo.Model('salt2-extended')\n",
    "mod_counter.parameters = SS.inverse_transform(explanation.cf['X'][0].reshape(1, -1))[0]\n",
    "sncosmo.plot_lc(data=None, bands=['desg','desr', 'desi', 'desz'], model=mod_counter);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9a092-1144-489e-91eb-fe7b36eb07c1",
   "metadata": {},
   "source": [
    "The scale seems roughly right, but the most obvious flaw is that the counterfactual supernova _dims_ instead of _brightening_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a2935-7b46-414f-a338-3b8a729371a4",
   "metadata": {},
   "source": [
    "## Challenge: Implement another classification method and generate another set of counterfactuals for a few inputs. Do some models have more naturally interpretable counterfactuals (and corresponding decision boundaries) than others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1275253-0a20-4664-9e71-6c459b58d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = # TODO: implement new classifier here, \n",
    "# Then, fit the model and generate a new counterfactual for the same example as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af08dce-fbfb-4cd9-b902-ec284aa3b49e",
   "metadata": {},
   "source": [
    "To get counterfactuals more closely matching the training dataset, we can use _prototypes_. This trick comes from a paper in 2019, [Interpretable Counterfactual Explanations Guided by Prototypes](https://arxiv.org/abs/1907.02584). By adding a term $L_{\\rm{proto}}$ to the loss function, we can guide our counterfactuals toward examples that fall within the data distribution _and_ differ from our sample class $i$. The additional term is defined as \n",
    "$$\n",
    "L_{\\rm{proto}} = \\theta \\;  || ENC(x+\\delta) - \\rm{proto}_j||_2^2\n",
    "$$\n",
    "where $i \\neq j$. This term penalizes counterfactuals far from a given prototype, with the distance measured in the latent space of the encoding. To make this work, however, we need a way of _encoding_ our counterfactual $x+\\delta$ and a pre-defined set of class prototypes in the same encoding space. How do we create these?\n",
    "\n",
    "Two examples from the 2019 paper (but surely not an exhaustive list) are:\n",
    "\n",
    "* Autoencoders\n",
    "* kd-trees\n",
    "\n",
    "We'll explore the second example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3496159-abc7-494b-a645-1599a925209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "cf = CounterfactualProto(predictor, shape, use_kdtree=True, theta=100., feature_range=(-1.e3, 1.e3))\n",
    "cf.fit(X_train, trustscore_kwargs=None)\n",
    "explanation = cf.explain(X[5].reshape(1, -1),  k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbde95-f8f4-4502-bd92-51564d25b76d",
   "metadata": {},
   "source": [
    "and now, let's investigate the counterfactual. What class is it predicted to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a946b-1910-4be5-9452-4f1b35e63aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = explanation.cf['class']\n",
    "print(f'Counterfactual prediction: {SNclasses[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284dd1-586c-4ed4-930d-32b85313f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and now, the counterfactual\n",
    "mod_counter.parameters = SS.inverse_transform(explanation.cf['X'][0].reshape(1, -1))[0]\n",
    "sncosmo.plot_lc(data=None, bands=['desg','desr', 'desi', 'desz'], model=[mod_orig, mod_counter], model_label=['Original Example', 'Counterfactual']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37485b-914c-4f0b-88d8-7f1b3e8e519f",
   "metadata": {},
   "source": [
    "Note that because we're using kdtrees, our prototypes are actual examples from our training set. In the autoencoder case, the prototypes are defined as the average of the k-nearest encodings of a given class $i \\neq j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb262d-4922-4fde-acec-f1efd3a8ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see how the input parameters differ; this is what needs to be changed to take us from an SN Ia to an SN II\n",
    "counter_params =  SS.inverse_transform(explanation.cf['X'][0].reshape(1, -1))[0]\n",
    "initial_params =  SS.inverse_transform(X[5].reshape(1, -1))[0]\n",
    "parm_diffs = counter_params - initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38822634-57b0-4991-bb03-0437362328e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SALT2 parameters: [z, t0, x0, x1, c]\")\n",
    "print(\"Absolute parameter changes\")\n",
    "print(parm_diffs)\n",
    "print(\"Fractional parameter changes\")\n",
    "print(parm_diffs/initial_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa3668-28e0-4cf5-9d03-603a15585594",
   "metadata": {},
   "source": [
    "These parameters are redshift (z), starting time (t0), amplitude (x0), light curve stretch (x1), and color (c). \n",
    "\n",
    "The counterfactual, then, has about the same redshift, but looks dimmer than the example event and starts earlier. Our counterfactual both looks like a supernova now, and has some desirable physical properties: type Ia supernovae are significantly brighter than SNe II on average, and we see this in our case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57746115-4fc0-4e9f-bad3-6ba3e84e2d20",
   "metadata": {},
   "source": [
    "We can also calculate counterfactuals guided by autoencoder-defined prototypes. \n",
    "# Challenge: Create an autoencoder for our galaxy images and generate a few counterfactuals for our galaxy classification model. Are the results more interpretable than the earlier examples? What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6298fba-0de0-4e80-8008-474c7784211d",
   "metadata": {},
   "source": [
    "Note - in the autoencoder prescription first outlined in the 2019 paper, the distance term in the loss function $\\lambda d(x, x')$ is replaced with the elastic net regularizer we saw on Day 1, and for the same reason: to ensure sparse & interpretable counterfactuals:\n",
    "$$\n",
    "\\lambda d(x, x') \\rightarrow \\beta ||x - x'||_1 + ||x-x'||_2^2\n",
    "$$\n",
    "Some helpful code is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cf437-9303-49fd-85b9-cdde5a524985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ae(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ae, self).__init__()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # TODO: Implement encoder\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        # TODO: Implement decoder\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "ae_model = ae().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ea637-302c-4cc0-9030-cfed39b0e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder you've just built\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ae_model.parameters(), lr=1.e-4)\n",
    "\n",
    "train_loss = []\n",
    "for epoch in trange(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = ae_model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "   train_loss.append(running_loss / len(train_loader.dataset))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff608d6-69d7-4795-ae68-9afe64317d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba911b5c-e0f1-40bd-a44c-fdacef9ee291",
   "metadata": {},
   "source": [
    "How well do we do on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ab776-262d-4a5c-9c86-111d079d19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image, _) = next(iter(test_loader))\n",
    "\n",
    "for i, img in enumerate(image):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(8, 4))\n",
    "    # Reshape the array for plotting\n",
    "    recon_img = ae_model(image[i]).detach().squeeze().numpy()\n",
    "    ax1.imshow(image[i].squeeze(), cmap='magma')\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax2.imshow(recon_img, cmap='magma')\n",
    "    ax2.set_title(\"Reconstruction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40d4f1-9744-4cb1-97ff-1b2ca8afb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#increase the batch size so that we have enough samples to pass to find the average encoder prototypes\n",
    "train_loader_proto = DataLoader(train, shuffle=True, batch_size=512, worker_init_fn=seed_worker, generator=g)\n",
    "images_proto, labels_proto = next(iter(train_loader_proto))\n",
    "\n",
    "# set some default parameters for the fit\n",
    "c_init = 1.\n",
    "c_steps = 1\n",
    "max_iterations = 100\n",
    "beta = #TODO    # elastic net regularization loss term\n",
    "gamma = #TODO  # autoencoder reconstruction loss term\n",
    "theta = #TODO  # prototype proximity loss term\n",
    "\n",
    "# initialize explainer\n",
    "shape = (1,) + image.shape[1:]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predictor(X: np.ndarray) -> np.ndarray:\n",
    "    X = torch.as_tensor(X,dtype=torch.float, device=device)\n",
    "    return model.forward(X).cpu().numpy()\n",
    "\n",
    "# initialize explainer, fit and generate counterfactual\n",
    "cf = CounterfactualProto(predictor, shape, beta=beta, gamma=gamma, theta=theta,\n",
    "                         ae_model=ae_model, enc_model=ae_model.encode, max_iterations=max_iterations,\n",
    "                         feature_range=(0, 1), c_init=c_init, c_steps=c_steps)\n",
    "\n",
    "start_time = time()\n",
    "cf.fit(images_proto)  # find class prototypes\n",
    "print('Time to find prototypes each class: {:.3f} sec'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b9bf8-5f90-4589-af22-d28a5fd368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy code from earlier to loop through a set of images and generate counterfactuals. \n",
    "# Note that the fitting routine may not always converge in the max number of iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ea13e-b209-4164-a94a-97e1410fde16",
   "metadata": {},
   "source": [
    "The differences now look physically meaningful, but they are also incredibly subtle. Is it more important to you to minimize the number of flipped pixels or to create a more physical representation of a prototype?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi_summerschool",
   "language": "python",
   "name": "iaifi_summerschool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
