{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815e09ff-704f-441f-8710-0351026c7b5d",
   "metadata": {},
   "source": [
    "# 4.3.  Gradient-Based Saliency Maps for Neural Network Interpretability\n",
    "### Alex Gagliano (gaglian2@mit.edu)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandergagliano/InterpretabilityDemos/blob/main/Tutorial%204.3%20PixelAttribution.ipynb)\n",
    "\n",
    "References and resources for additional reading:\n",
    "* [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034), Simonyan et al., 2013 - implemented [here](https://github.com/sunnynevarekar/pytorch-saliency-maps/blob/master/Saliency_maps_in_pytorch.ipynb).\n",
    "* [Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901), Zeiler et al. (2013) \n",
    "* [Striving for Simplicity - The All Convolutional Net](http://arxiv.org/abs/1412.6806), Springenberg et al. (2015)\n",
    "* [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825), Smilkov et al. (2017)\n",
    "* [https://github.com/wangyongjie-ntu/Awesome-explainable-AI]().\n",
    "\n",
    "Saliency maps are potentially the most commonly-used approaches for pixel attribution in image-based neural networks (pixel attribution = sensitivity maps = saliency maps). Introduced in their first form by Simonyan et al. (2013), they were first generated by calculating the gradients of the loss function for a given output class with respect to the image pixels (holding the weights fixed). They're straightforward to calculate, so let's do that now with the same galaxy classification model we trained in the previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d1b5b-f7dd-4142-9a41-aa14ee344b64",
   "metadata": {},
   "source": [
    "One final time, let's start by installing some necessary packages and downloading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea3f7b-80d3-4090-9243-91a4cc2e8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi torch torchvision gdown shap\n",
    "\n",
    "!gdown 1s42ri7tpvBHN-kneC0MHXHsd2nB2BNTE\n",
    "!gdown 1CC07axEZravXcIkq0w2gnkFGGhO4vbnx\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import subprocess\n",
    "import shap \n",
    "\n",
    "for file in ['data', 'models']:\n",
    "    subprocess.run([\"tar\", \"-xf\", \"%s.tar.gz\" % file]);\n",
    "    \n",
    "galaxyPath = './data/galaxy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1aa8c-c3cd-4cf3-9136-c69cb4c2d2d0",
   "metadata": {},
   "source": [
    "## 4.3.1. Saliency Maps for Galaxy Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804df2-bae2-4174-828f-89edf2f050dc",
   "metadata": {},
   "source": [
    "We'll now re-define our galaxy classification model and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b88b79-fc87-454a-8b19-453e146e0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 20, kernel_size=5),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(720, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(50, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3*16*15)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "#load the model from the SHAP tutorial!\n",
    "model.load_state_dict(torch.load('./models/galaxyCNN_legacy.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9aee7-f69a-4ace-a6e5-20f15ea637e0",
   "metadata": {},
   "source": [
    "and now, running a forward pass for predictions and a backward pass to get our gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c037d98-bf7c-44c5-961a-1c0839384d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GalaxyClasses = {0:'Ellip.', 1:'Spiral', 2:'Irreg.'}\n",
    "\n",
    "def vanilla_saliency(input, model):\n",
    "    # We don't need gradients with respect to the weights, so we can turn these off\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # We want to calculate gradients of the input,\n",
    "    # so set requires_grad to True for input \n",
    "    input.requires_grad = True\n",
    "    \n",
    "    # Forward pass to calculate predictions\n",
    "    preds = model(input)\n",
    "    score, indices = torch.max(preds, 1)\n",
    "    \n",
    "    # Backward pass to get gradients of score predicted class w.r.t. input image\n",
    "    score.backward()\n",
    "    slc = input.grad[0]\n",
    "    return slc.numpy()\n",
    "\n",
    "def plot_saliency(input, saliency_set, include_shap=False, labels=['Basic', 'Guided', 'SmoothGrad', 'SHAP']):\n",
    "    ncol = len(saliency_set)+1\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 20))\n",
    "    if include_shap:\n",
    "        ncol += 1\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                    nrows_ncols = (1,ncol),\n",
    "                    axes_pad = 0.05\n",
    "                    )\n",
    "    \n",
    "    grid[0].imshow(input.detach().numpy().squeeze(0))\n",
    "    grid[0].axis('off')\n",
    "    grid[0].set_title('Input Image')\n",
    "\n",
    "    for i in np.arange(len(saliency_set)):\n",
    "        slc = saliency_set[i]\n",
    "        \n",
    "        #normalize to [0..1]\n",
    "        slc = np.abs(slc)\n",
    "        slc = (slc - slc.min())/(slc.max()-slc.min())\n",
    "        \n",
    "        imc = grid[i+1].imshow(slc, cmap='hot',  vmin=0, vmax=1)\n",
    "        grid[i+1].axis('off')\n",
    "        grid[i+1].set_title(labels[i])  \n",
    "    if include_shap:\n",
    "        return grid\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bee40-2912-4781-94e9-f2c134d25655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(50)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data_transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                     transforms.ToTensor()])\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(os.path.join(galaxyPath, 'train'), transform = data_transform)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_size, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "test = torchvision.datasets.ImageFolder(os.path.join(galaxyPath, 'test'), transform = data_transform)\n",
    "test_loader = DataLoader(test, shuffle=True, batch_size=batch_size, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72d5c9-49fc-4f35-b72c-73a6f4be4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "for i in np.arange(batch_size):\n",
    "    map = vanilla_saliency(imgs[i], model)\n",
    "    plot_saliency(imgs[i], [map], labels=['Vanilla'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2da7b1-f474-4bc6-8f7f-144e45be9ffb",
   "metadata": {},
   "source": [
    "The saliency maps look not-too-different in our 32x32 pixel image set, especially since our model is small and we trained it quickly. But we can begin to see the pixels that are used to determine the morphological classification of each galaxy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fb977-12b3-421f-bb9e-ede98f98da92",
   "metadata": {},
   "source": [
    "One of the limitations of these \"vanilla' gradients for interpretability is that we're seeing the pixel gradients propagated through multiple hidden layers, so the gradients can cancel each other out. Further, the pixels with negative gradients show us features of the image that \"suppress\" network neurons. If we want to focus on what pixels had high importance for classification, we can zero out the negative gradients during backpropagation with a reLU function. This is called _guided backpropogation_ (Springenberg et al., 2015):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef323528-e99c-45c5-acbb-4133931ef824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a hook into our network that clamps the backward gradient values, such that they can only be positive\n",
    "def relu_backward_hook_fn(module, grad_in, grad_out):\n",
    "    return (torch.clamp(grad_in[0], min=0.0), )\n",
    "    \n",
    "def setup_hooks(model):    \n",
    "    for module in model.modules():\n",
    "        if type(module) == nn.ReLU:\n",
    "            module.register_full_backward_hook(relu_backward_hook_fn)\n",
    "    return model\n",
    "    \n",
    "def guided_saliency(input, model):\n",
    "    # We don't need gradients with respect to the weights, so we can turn these off\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # We want to calculate gradients of the input,\n",
    "    # so set requires_grad to True for input \n",
    "    input.requires_grad = True\n",
    "    \n",
    "    # Forward pass to calculate predictions\n",
    "    preds = model(input)\n",
    "    score, indices = torch.max(preds, 1)\n",
    "    \n",
    "    # Backward pass to get gradients of score predicted class w.r.t. input image\n",
    "    score.backward()\n",
    "    slc = input.grad[0]\n",
    "    return slc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c9cbc-bfc0-494b-94df-84fbb0972860",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModel = setup_hooks(copy.deepcopy(model))\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "for i in np.arange(batch_size):\n",
    "    map1 = vanilla_saliency(imgs[i], model)\n",
    "    map2 = guided_saliency(imgs[i], newModel)\n",
    "    plot_saliency(imgs[i], [map1, map2])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f19275-e4b5-42d2-94ad-a88a92bac052",
   "metadata": {},
   "source": [
    "Potentially we do a *bit* better in some images, as we get rid of some of the more irrelevant pixels and focus in on some of the more significant ones. Across the two methods, the techniques still look pretty noisy. [Smilkov et al. (2017)](https://arxiv.org/pdf/1706.03825.pdf) propose that this is due to meaningless local variations in the calculated variations. Their `SmoothGrad` technique for reducing this noise is motivated by our earlier discussion of ensemble techniques, and is wonderfully simple: for an input image with pixels $x$, and calculated saliency map $M(x)$, a lower-noise saliency map can be calculated as\n",
    "$$\n",
    "M'(x) = \\frac{1}{n}\\sum_1^n M(x + \\mathcal{N}(0, \\sigma^2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30713041-15cb-44c9-8680-94ce36b97eb6",
   "metadata": {},
   "source": [
    "In summary: Create $n$ versions of the input image, add noise to each pixel from a distribution with variance $\\sigma^2$, and average them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f373d6c-12b5-475f-8442-62b74b3a7689",
   "metadata": {},
   "source": [
    "## Challenge: Implement the SmoothGrad technique by modifying an earlier function. \n",
    "Explore different values of $\\sigma$ and $N$ and explore how this impacts the resulting saliency map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef9811-b888-4645-a355-93baa45d41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad_saliency(input, model, sigma, N):\n",
    "    # We don't need gradients with respect to the weights, so we can turn these off\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #add gaussian noise: \n",
    "    map_set = []\n",
    "    total_gradients = torch.zeros(32, 32)\n",
    "    for i in np.arange(N):\n",
    "        # TODO: add Gaussian noise with std=sigma\n",
    "        # to our input image to generate \n",
    "        # input_noisy. Then, compute a forward\n",
    "        # and backward pass on the image\n",
    "        # as before.\n",
    "        total_gradients += input_noisy.grad[0]\n",
    "    return total_gradients.numpy() / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a1600-4b25-4feb-8afa-9fe834eb5203",
   "metadata": {},
   "source": [
    "## 4.3.2. A comparison to SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823b326-bdfa-4b32-aa96-397e312a2398",
   "metadata": {},
   "source": [
    "For comparison, I'll throw up the SHAP estimates for these pixels as well. This time, we'll only show the Shapley estimates for the class with the highest prediction for the model (same as is done for the gradient-based saliency maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10578b-e778-412a-bade-89a44f3aaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_size = 30\n",
    "background_loader = DataLoader(test, shuffle=True, batch_size=background_size)\n",
    "background_batch = next(iter(background_loader))\n",
    "background_images, background_labels = background_batch\n",
    "\n",
    "e = shap.DeepExplainer(model, background_images)\n",
    "shap_values = e.shap_values(imgs)\n",
    "shap_values = np.array(shap_values).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015f7a0-d667-4254-93f2-4a5fa40d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(imgs)\n",
    "score, indices = torch.max(preds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0e027-6221-4647-970c-386a8a8c947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_max = []\n",
    "for idx, chosenClass in enumerate(indices.numpy()):\n",
    "    shap_values_max.append(shap_values[chosenClass, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0007527-93d0-47f7-9bf3-3c991bde9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(batch_size):\n",
    "    map1 = vanilla_saliency(imgs[i], model)\n",
    "    map2 = guided_saliency(imgs[i], newModel)\n",
    "    map3 = smoothgrad_saliency(imgs[i], model, sigma=0.02, N=300)\n",
    "    plot_saliency(imgs[i], [map1, map2, map3, shap_values_max[i]])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f090ec6-924b-402e-93af-90b19f79fad4",
   "metadata": {},
   "source": [
    "Notice that because SHAP computes feature contribution relative to some background \"baseline\", it pulls out the anomalous features most clearly (like a bar, an off-center galaxy, etc). But these might not be all the features that the model uses for classification on other features! Gradients manage to present a more general picture of what a model uses for its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f7894-fa96-430c-9580-f698ab9ab0b7",
   "metadata": {},
   "source": [
    "Of course, with larger networks, larger training sets, and more training epochs, saliency maps can become quite effective (here's a few examples from the [SmoothGrad paper](https://arxiv.org/abs/1706.03825)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7b1b7-e863-4033-846d-7f98c7c92017",
   "metadata": {},
   "source": [
    "![](images/SmoothGrad_SaliencyMaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f5753-c156-435f-b543-b7f4d181dfad",
   "metadata": {},
   "source": [
    "One final trick: to more easily resolve structure from the original image, it has become common to try multiplying the backpropagated gradients by the original image pixels. \n",
    "# Challenge: Implement Gradient x Image versions of the four methods used above. \n",
    "How do they compare to our earlier results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763b2e3-b143-4a3b-af5b-1b3bb9162b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(batch_size):\n",
    "    basicgrad = vanilla_saliency(imgs[i], model)\n",
    "    guidedgrad = guided_saliency(imgs[i], newModel)\n",
    "    smoothgrad = smoothgrad_saliency(imgs[i], model, sigma=0.02, N=300)\n",
    "\n",
    "    # TODO: multiply your input images and saliency maps.\n",
    "    # do you have to pre-process your images so that their contributions\n",
    "    # are equally weighted? \n",
    "    \n",
    "    # Plot the resulting saliency maps. Does the structure increase? \n",
    "    plot_saliency(imgs[i], [basicgrad_new, guidedimg_new, smoothgrad_new, shap_new], labels=['BasicxImg', 'GuidedxImg', 'SmoothGradxImg', 'SHAPxImg'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84502b99-82f3-450e-91d3-cbd7b11b99bb",
   "metadata": {},
   "source": [
    "And SmoothGrad's corresponding maps:\n",
    "\n",
    "\n",
    "![](images/SmoothGrad_xSaliencyMaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb4c9d-5f21-4d71-8b7a-95040ed9e1a6",
   "metadata": {},
   "source": [
    "There are more complicated pixel attribution methods, but that's all we'll explore here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48068233-cbf3-4722-a6a2-45cfa010631f",
   "metadata": {},
   "source": [
    "In summary, there are global interpretability methods (like the permutation importance of gradient boosting) and local interpretability methods (like SHAP and counterfactuals). There are model-agnostic methods (like SHAP) and model-specific methods (like guided backpropagation for pixel attribution). Each of these presents a slightly different picture of your model. When used togther, they provide a powerful toolkit for understanding what's going on under the hood.\n",
    "\n",
    "Beyond pixel attribution, there are several other methods that can be useful to better understand the behavior of a neural network. One is feature visualization, which tries to generate archetypes for what components of a neural network look for in an image. This is done by generating images that maximize the response of individual neurons, layers, or layer groups (shown here for the massive GoogLeNet network):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdf507-4881-4ba8-b2d0-7dc18637fcbc",
   "metadata": {},
   "source": [
    "![](images/FeatureVisualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bb7ba-59fd-4991-8ee8-79c2ae5b7f43",
   "metadata": {},
   "source": [
    "More information on feature visualization techniques (and some great associated notebooks) can be found at [https://distill.pub/2017/feature-visualization/](https://distill.pub/2017/feature-visualization/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
